{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KatBCN/MUD-Lab-DDI-NN/blob/main/experiments/lstm_context/MUDLab6_DDI_LSTMcontext.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MUD Lab Session 6 - DDI Using Neural Networks\n",
        "\n",
        "Change Runtime Type to GPU before running."
      ],
      "metadata": {
        "id": "tpJid7hPK-gG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mX7VXCjWfKeQ"
      },
      "outputs": [],
      "source": [
        "#!pip install tensorflow_addons"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "# mount drive\n",
        "drive.mount('/content/drive/')\n",
        "%cd /content/drive/MyDrive/MUD-Lab-6\n",
        "%ls "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zxFamfkLG7i",
        "outputId": "57a1ff54-a804-457c-d2ba-132fabe82c10"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "/content/drive/MyDrive/MUD-Lab-6\n",
            "codemaps_lw_pos.py  \u001b[0m\u001b[01;34mdata\u001b[0m/       devel.out     \u001b[01;34mExperiments\u001b[0m/  \u001b[01;34m__pycache__\u001b[0m/\n",
            "codemaps_pos.py     dataset.py  devel.stats   \u001b[01;34mmodel\u001b[0m/        \u001b[01;34mutil\u001b[0m/\n",
            "codemaps.py         deptree.py  evaluator.py  model.idx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BowuMcDbdnMe"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import time\n",
        "sys.path.insert(1, \"/content/drive/MyDrive/MUD-Lab-6\")\n",
        "\n",
        "import random\n",
        "from contextlib import redirect_stdout\n",
        "\n",
        "from tensorflow.keras import regularizers, Input\n",
        "from tensorflow.keras.models import Model,load_model\n",
        "from tensorflow.keras.layers import Embedding, Dense, Dropout, Conv1D, MaxPool1D, Reshape, Concatenate, Flatten, Bidirectional, LSTM\n",
        "\n",
        "\n",
        "from deptree import *\n",
        "from dataset import *\n",
        "from codemaps import *\n",
        "import evaluator\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(23)\n",
        "random.seed(23)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment\n",
        "\n",
        "\n",
        "- Conv1D: filters=50 (baseline was 30)\n",
        "- Changing the vector that flattens the convolutional output by concatenating the vectors to an LSTM context vector\n",
        "  - dropout=0.0 in LSTM layer\n",
        "\n",
        "\n",
        "\n",
        "[Keras Reference: LSTM layer](https://keras.io/api/layers/recurrent_layers/lstm/)"
      ],
      "metadata": {
        "id": "U6ZPdbkcLYUt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EVs3b3mChb01"
      },
      "outputs": [],
      "source": [
        "#this is building the neural network.\n",
        "def build_network(idx) :\n",
        "\n",
        "   # sizes\n",
        "   n_words = codes.get_n_words() # number of words in vocabulary\n",
        "   max_len = codes.maxlen\n",
        "   n_labels = codes.get_n_labels()\n",
        "\n",
        "   # word input layer & embeddings\n",
        "   inptW = Input(shape=(max_len,)) # shape of input: all vectors need to be the same length.\n",
        "   embW = Embedding(input_dim=n_words, output_dim=100, # output_dim is the hyperparameter that can be tuned.\n",
        "                      input_length=max_len, mask_zero=False)(inptW)  \n",
        "\n",
        "  # number of filters it the output dimension which can be tuned.\n",
        "   conv = Conv1D(filters=50, kernel_size=2, strides=1, activation='relu', padding='same')(embW) # kernel size is the context of words, depending on the stride, you can have overlapping.\n",
        "   # flat= Flatten()(conv) # concatenating the vectors one after the other - we can change this architecture.\n",
        "   lstm = LSTM(units=50,dropout=0.0)(conv) # context vector - adding dropout to regularize.\n",
        "\n",
        "   # we need a single vector\n",
        "   # out = Dense(n_labels, activation='softmax')(flat)\n",
        "   out = Dense(n_labels, activation='softmax')(lstm)\n",
        "\n",
        "   model = Model(inptW, out)\n",
        "   model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "   return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZuXt4Sf1ifKc"
      },
      "outputs": [],
      "source": [
        "# directory with files to process\n",
        "trainfile = \"/content/drive/MyDrive/MUD-Lab-6/data/train.pck\"\n",
        "validationfile = \"/content/drive/MyDrive/MUD-Lab-6/data/devel.pck\"\n",
        "modelname = \"model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tN5elfAkhjqf",
        "outputId": "60a95eac-002b-449e-929b-3f1128f57afd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 150)]             0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 150, 100)          455300    \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 150, 50)           10050     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 50)                20200     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 5)                 255       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 485,805\n",
            "Trainable params: 485,805\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "724/724 [==============================] - 21s 11ms/step - loss: 0.6105 - accuracy: 0.8521 - val_loss: 0.6319 - val_accuracy: 0.8393\n",
            "Epoch 2/10\n",
            "724/724 [==============================] - 7s 9ms/step - loss: 0.5902 - accuracy: 0.8532 - val_loss: 0.6180 - val_accuracy: 0.8393\n",
            "Epoch 3/10\n",
            "724/724 [==============================] - 7s 9ms/step - loss: 0.5460 - accuracy: 0.8532 - val_loss: 0.5533 - val_accuracy: 0.8393\n",
            "Epoch 4/10\n",
            "724/724 [==============================] - 7s 9ms/step - loss: 0.4669 - accuracy: 0.8542 - val_loss: 0.5114 - val_accuracy: 0.8317\n",
            "Epoch 5/10\n",
            "724/724 [==============================] - 6s 9ms/step - loss: 0.3605 - accuracy: 0.8769 - val_loss: 0.5432 - val_accuracy: 0.8406\n",
            "Epoch 6/10\n",
            "724/724 [==============================] - 6s 9ms/step - loss: 0.3081 - accuracy: 0.8888 - val_loss: 0.5242 - val_accuracy: 0.8497\n",
            "Epoch 7/10\n",
            "724/724 [==============================] - 7s 9ms/step - loss: 0.2762 - accuracy: 0.8963 - val_loss: 0.5251 - val_accuracy: 0.8419\n",
            "Epoch 8/10\n",
            "724/724 [==============================] - 6s 9ms/step - loss: 0.2475 - accuracy: 0.9061 - val_loss: 0.5275 - val_accuracy: 0.8219\n",
            "Epoch 9/10\n",
            "724/724 [==============================] - 7s 9ms/step - loss: 0.2253 - accuracy: 0.9191 - val_loss: 0.5128 - val_accuracy: 0.8388\n",
            "Epoch 10/10\n",
            "724/724 [==============================] - 7s 9ms/step - loss: 0.2034 - accuracy: 0.9291 - val_loss: 0.4844 - val_accuracy: 0.8325\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: model/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: model/assets\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f64d09e6790> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ],
      "source": [
        "## --------- MAIN PROGRAM ----------- \n",
        "## --\n",
        "## -- Usage:  train.py ../data/Train ../data/Devel  modelname\n",
        "## --\n",
        "\n",
        "## --------- MAIN PROGRAM ----------- \n",
        "## --\n",
        "## -- Usage:  train.py ../data/Train ../data/Devel  modelname\n",
        "## --\n",
        "\n",
        "\n",
        "# directory with files to process\n",
        "# trainfile = sys.argv[1]\n",
        "# validationfile = sys.argv[2]\n",
        "# modelname = sys.argv[3]\n",
        "\n",
        "# load train and validation data\n",
        "traindata = Dataset(trainfile)\n",
        "valdata = Dataset(validationfile)\n",
        "\n",
        "# create indexes from training data\n",
        "max_len = 150\n",
        "suf_len = 5\n",
        "codes = Codemaps(traindata, max_len)\n",
        "\n",
        "# build network\n",
        "model = build_network(codes)\n",
        "with redirect_stdout(sys.stderr) :\n",
        "   model.summary()\n",
        "\n",
        "# encode datasets\n",
        "Xt = codes.encode_words(traindata)\n",
        "Yt = codes.encode_labels(traindata)\n",
        "Xv = codes.encode_words(valdata)\n",
        "Yv = codes.encode_labels(valdata)\n",
        "\n",
        "#track time to train model\n",
        "start_time = time.time()\n",
        "# train model\n",
        "with redirect_stdout(sys.stderr) :\n",
        "   model.fit(Xt, Yt, batch_size=32, epochs=10, validation_data=(Xv,Yv), verbose=1) # changing number of epochs fewer to prevent overfitting.\n",
        "\n",
        "train_time = time.time() - start_time\n",
        "\n",
        "# save model and indexs\n",
        "model.save(modelname)\n",
        "codes.save(modelname)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Amount of time to train the model is:\", train_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMcRL_3gOSBs",
        "outputId": "f5a3ca40-7e5a-4b93-ff94-03f6680e93e7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Amount of time to train the model is: 80.13637089729309\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fname = \"/content/drive/MyDrive/MUD-Lab-6/model\"\n",
        "datafile = \"/content/drive/MyDrive/MUD-Lab-6/data/devel.pck\"\n",
        "outfile = \"devel.out\""
      ],
      "metadata": {
        "id": "jq2RfnjNpFmc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code chunk is from predict.py"
      ],
      "metadata": {
        "id": "SXlLXmYxoR7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# predict.py\n",
        "\n",
        "## --------- Entity extractor ----------- \n",
        "## -- Extract drug entities from given text and return them as\n",
        "## -- a list of dictionaries with keys \"offset\", \"text\", and \"type\"\n",
        "\n",
        "def output_interactions(data, preds, outfile) :\n",
        "\n",
        "   #print(testdata[0])\n",
        "   outf = open(outfile, 'w')\n",
        "   for exmp,tag in zip(data.sentences(),preds) :\n",
        "      sid = exmp['sid']\n",
        "      e1 = exmp['e1']\n",
        "      e2 = exmp['e2']\n",
        "      if tag!='null' :\n",
        "         print(sid, e1, e2, tag, sep=\"|\", file=outf)\n",
        "            \n",
        "   outf.close()\n",
        "\n",
        "   \n",
        "## --------- MAIN PROGRAM ----------- \n",
        "## --\n",
        "## -- Usage:  baseline-NER.py target-dir\n",
        "## --\n",
        "## -- Extracts Drug NE from all XML files in target-dir\n",
        "## --\n",
        "\n",
        "#fname = sys.argv[1]\n",
        "#datafile = sys.argv[2]\n",
        "#outfile = sys.argv[3]\n",
        "\n",
        "model = load_model(fname)\n",
        "codes = Codemaps(fname)\n",
        "\n",
        "testdata = Dataset(datafile)\n",
        "X = codes.encode_words(testdata)\n",
        "\n",
        "Y = model.predict(X)\n",
        "Y = [codes.idx2label(np.argmax(s)) for s in Y]\n",
        "\n",
        "# extract relations\n",
        "output_interactions(testdata, Y, outfile)\n"
      ],
      "metadata": {
        "id": "vHXEGZNkn3Rt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task = \"DDI\"\n",
        "golddir = \"/content/drive/MyDrive/MUD-Lab-6/data/devel\"\n",
        "outfile = \"devel.out\""
      ],
      "metadata": {
        "id": "Vb9e-qHwrpaC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code chunk is from evaluator.py and creates file devel.stats"
      ],
      "metadata": {
        "id": "JOIWKxLpoisY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluator.py\n",
        "\n",
        "#! /usr/bin/python3\n",
        "\n",
        "import sys\n",
        "from os import listdir\n",
        "\n",
        "from xml.dom.minidom import parse\n",
        "\n",
        "## --\n",
        "## -- auxliary to insert an instance in given instance_set\n",
        "## --\n",
        "\n",
        "def add_instance(instance_set, einfo, etype) :\n",
        "    instance_set[\"CLASS\"].add(einfo+\"|\"+etype)\n",
        "    instance_set[\"NOCLASS\"].add(einfo)\n",
        "    if etype not in instance_set : instance_set[etype] = set([])\n",
        "    instance_set[etype].add(einfo)\n",
        "\n",
        "    \n",
        "## --\n",
        "## -- Load entities from XML files in given golddir\n",
        "## --\n",
        "\n",
        "\"\"\"\n",
        "def load_gold_NER(golddir) :\n",
        "    entities = { \"CLASS\" : set([]), \"NOCLASS\" : set([]) }\n",
        "\n",
        "    # process each file in directory\n",
        "    for f in listdir(golddir) :\n",
        "\n",
        "        # parse XML file, obtaining a DOM tree\n",
        "        tree = parse(golddir+\"/\"+f)\n",
        "\n",
        "        # process each sentence in the file\n",
        "        sentences = tree.getElementsByTagName(\"sentence\")\n",
        "        for s in sentences :\n",
        "            sid = s.attributes[\"id\"].value   # get sentence id\n",
        "            \n",
        "            # load sentence entities\n",
        "            ents = s.getElementsByTagName(\"entity\")\n",
        "            for e in ents :\n",
        "                einfo = sid + \"|\" + e.attributes[\"charOffset\"].value  + \"|\" + e.attributes[\"text\"].value\n",
        "                etype = e.attributes[\"type\"].value\n",
        "                add_instance(entities, einfo, etype)\n",
        "            \n",
        "    return entities\n",
        "\"\"\"\n",
        "## --\n",
        "## -- Load relations from XML files in given golddir\n",
        "## --\n",
        "\n",
        "def load_gold_DDI(golddir) :\n",
        "    relations = { \"CLASS\" : set([]), \"NOCLASS\" : set([]) }\n",
        "\n",
        "    # process each file in directory\n",
        "    for f in listdir(golddir) :\n",
        "\n",
        "        # parse XML file, obtaining a DOM tree\n",
        "        tree = parse(golddir+\"/\"+f)\n",
        "\n",
        "        # process each sentence in the file\n",
        "        sentences = tree.getElementsByTagName(\"sentence\")\n",
        "        for s in sentences :\n",
        "            sid = s.attributes[\"id\"].value   # get sentence id\n",
        "        \n",
        "            # load \"pairs\"  in the sentence, keep those with ddi=true\n",
        "            pairs = s.getElementsByTagName(\"pair\")\n",
        "            for p in pairs:\n",
        "                id_e1 = p.attributes[\"e1\"].value\n",
        "                id_e2 = p.attributes[\"e2\"].value\n",
        "                ddi = p.attributes[\"ddi\"].value\n",
        "\n",
        "                if (ddi == \"true\") :\n",
        "                    rtype = p.attributes[\"type\"].value\n",
        "                    rinfo = sid + \"|\" + id_e1 + \"|\" +  id_e2\n",
        "                    add_instance(relations, rinfo, rtype)\n",
        "\n",
        "    return relations\n",
        "\n",
        "\n",
        "## --\n",
        "## -- Load entities/relations from given system output file\n",
        "## --\n",
        "\n",
        "def load_predicted(task, outfile) :\n",
        "    predicted = { \"CLASS\" : set([]), \"NOCLASS\" : set([]) }\n",
        "    outf = open(outfile,\"r\")\n",
        "    for line in outf.readlines() :\n",
        "        line = line.strip()\n",
        "        if line in predicted[\"CLASS\"] :\n",
        "            print(\"Ignoring duplicated entity in system predictions file: \"+line)\n",
        "            continue\n",
        "\n",
        "        etype = line.split(\"|\")[-1]\n",
        "        einfo = \"|\".join(line.split(\"|\")[:-1])\n",
        "        add_instance(predicted, einfo, etype)\n",
        "        outf.close()\n",
        "        \n",
        "    return predicted\n",
        "    \n",
        "\n",
        "\n",
        "## --\n",
        "## -- Compare given sets and compute tp,fp,fn,P,R,F1\n",
        "## --\n",
        "\n",
        "def statistics(gold,predicted,kind) :\n",
        "    tp = 0\n",
        "    fp = 0\n",
        "    nexp = len(gold[kind])\n",
        "    if kind in predicted:\n",
        "        npred = len(predicted[kind])\n",
        "        for p in predicted[kind] :\n",
        "            if p in gold[kind] : tp += 1\n",
        "            else : fp += 1\n",
        "\n",
        "        fn = 0\n",
        "        for p in gold[kind] :\n",
        "            if p not in predicted[kind] : fn += 1\n",
        "\n",
        "    else :\n",
        "        npred = 0\n",
        "        fn = nexp\n",
        "\n",
        "    P = tp/npred if npred!=0 else 0\n",
        "    R = tp/nexp if nexp!=0 else 0    \n",
        "    F1 = 2*P*R/(P+R) if P+R!=0 else 0\n",
        "\n",
        "    return tp,fp,fn,npred,nexp,P,R,F1\n",
        "\n",
        "## --\n",
        "## -- Compute and print statistics table\n",
        "## --\n",
        "\n",
        "def row(txt) :\n",
        "   return txt + ' '*(17-len(txt))\n",
        "\n",
        "\n",
        "def print_statistics(gold,predicted, statfile) :\n",
        "    print(row(\"\")+\"  tp\\t  fp\\t  fn\\t#pred\\t#exp\\tP\\tR\\tF1\", file=statfile)\n",
        "    print(\"------------------------------------------------------------------------------\", file=statfile)\n",
        "    (nk,sP,sR,sF1) = (0,0,0,0)\n",
        "    for kind in sorted(gold) :\n",
        "        if kind==\"CLASS\" or kind==\"NOCLASS\" : continue\n",
        "        (tp,fp,fn,npred,nexp,P,R,F1) = statistics(gold, predicted, kind)\n",
        "        print(row(kind)+\"{:>4}\\t{:>4}\\t{:>4}\\t{:>4}\\t{:>4}\\t{:2.1%}\\t{:2.1%}\\t{:2.1%}\".format(tp,fp,fn,npred,nexp, P, R, F1), file=statfile)\n",
        "        (nk,sP,sR,sF1) = (nk+1, sP+P, sR+R, sF1+F1)\n",
        "\n",
        "    (sP, sR, sF1) = (sP/nk, sR/nk, sF1/nk)\n",
        "    print(\"------------------------------------------------------------------------------\", file=statfile)\n",
        "    print(row(\"M.avg\")+\"-\\t-\\t-\\t-\\t-\\t{:2.1%}\\t{:2.1%}\\t{:2.1%}\".format(sP, sR, sF1), file=statfile)\n",
        "\n",
        "    print(\"------------------------------------------------------------------------------\", file=statfile)\n",
        "    (tp,fp,fn,npred,nexp,P,R,F1) = statistics(gold, predicted, \"CLASS\")\n",
        "    print(row(\"m.avg\")+\"{:>4}\\t{:>4}\\t{:>4}\\t{:>4}\\t{:>4}\\t{:2.1%}\\t{:2.1%}\\t{:2.1%}\".format(tp,fp,fn,npred,nexp, P, R, F1), file=statfile)                        \n",
        "    (tp,fp,fn,npred,nexp,P,R,F1) = statistics(gold, predicted, \"NOCLASS\")\n",
        "    print(row(\"m.avg(no class)\")+\"{:>4}\\t{:>4}\\t{:>4}\\t{:>4}\\t{:>4}\\t{:2.1%}\\t{:2.1%}\\t{:2.1%}\".format(tp,fp,fn,npred,nexp, P, R, F1), file=statfile)               \n",
        "\n",
        "## --\n",
        "## -- Evaluates results in outfile comparing them with gold standard in golddir.\n",
        "## -- 'task' is either NER or DDI\n",
        "## -- This function can be called from any program requesting evaluation.\n",
        "## --\n",
        " \n",
        "def evaluate(task, golddir, outfile):\n",
        "\n",
        "    if task==\"NER\" :\n",
        "        # get set of expected entities in the whole golddir\n",
        "        gold = load_gold_NER(golddir)\n",
        "    elif task == \"DDI\" :\n",
        "        # get set of expected relations in the whole golddir\n",
        "        gold = load_gold_DDI(golddir)\n",
        "    else :\n",
        "        print (\"Invalid task '\"+task+\"'. Please specify 'NER' or 'DDI'.\")        \n",
        "\n",
        "\n",
        "    # Load entities/relations predicted by the system\n",
        "    predicted = load_predicted(task, outfile)\n",
        "\n",
        "    # compare both sets and compute statistics\n",
        "    statfile = open(\"devel.stats\", 'w')\n",
        "    print_statistics(gold,predicted, statfile)\n",
        "    statfile.close()\n",
        "\n",
        "         \n",
        "        \n",
        "## --\n",
        "## -- Usage as standalone program:  evaluator.py (NER|DDI) golddir outfile\n",
        "## --\n",
        "## -- Evaluates results in outfile comparing them with gold standard in golddir\n",
        "## --\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    #if len(sys.argv) != 4 :\n",
        "     #   print(\"\\n  Usage: evaluator.py (NER|DDI) golddir outfile\\n\")\n",
        "      #  exit()\n",
        "        \n",
        "    task = \"DDI\"\n",
        "    golddir = \"/content/drive/MyDrive/MUD-Lab-6/data/devel\"\n",
        "    outfile = \"devel.out\"\n",
        "\n",
        "    evaluate(task, golddir, outfile)\n"
      ],
      "metadata": {
        "id": "hOK3PizbohBR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results with LSTM context, dropout=0.0\n",
        "\n",
        "The F1 score for all categories and the overall F1 score is worse when adding the LSTM context vector instead of the flatten vector.\n"
      ],
      "metadata": {
        "id": "hLt4nn9CRpO1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "devel.stats:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "                   tp\t  fp\t  fn\t#pred\t#exp\tP\tR\tF1\n",
        "------------------------------------------------------------------------------\n",
        "advise             52\t  49\t  89\t 101\t 141\t51.5%\t36.9%\t43.0%\n",
        "effect            167\t 285\t 145\t 452\t 312\t36.9%\t53.5%\t43.7%\n",
        "int                12\t  10\t  16\t  22\t  28\t54.5%\t42.9%\t48.0%\n",
        "mechanism          38\t 163\t 223\t 201\t 261\t18.9%\t14.6%\t16.5%\n",
        "------------------------------------------------------------------------------\n",
        "M.avg            -\t-\t-\t-\t-\t40.5%\t37.0%\t37.8%\n",
        "------------------------------------------------------------------------------\n",
        "m.avg             269\t 507\t 473\t 776\t 742\t34.7%\t36.3%\t35.4%\n",
        "m.avg(no class)   476\t 300\t 266\t 776\t 742\t61.3%\t64.2%\t62.7%\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BZMWmQBjRgon"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline Results for Comparison:"
      ],
      "metadata": {
        "id": "paHfdeP9Jac1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "devel.stats:\n",
        "\n",
        "```\n",
        "                   tp\t  fp\t  fn\t#pred\t#exp\tP\tR\tF1\n",
        "------------------------------------------------------------------------------\n",
        "advise             90\t  54\t  51\t 144\t 141\t62.5%\t63.8%\t63.2%\n",
        "effect            160\t  98\t 152\t 258\t 312\t62.0%\t51.3%\t56.1%\n",
        "int                14\t   5\t  14\t  19\t  28\t73.7%\t50.0%\t59.6%\n",
        "mechanism          64\t  53\t 197\t 117\t 261\t54.7%\t24.5%\t33.9%\n",
        "------------------------------------------------------------------------------\n",
        "M.avg            -\t-\t-\t-\t-\t63.2%\t47.4%\t53.2%\n",
        "------------------------------------------------------------------------------\n",
        "m.avg             328\t 210\t 414\t 538\t 742\t61.0%\t44.2%\t51.2%\n",
        "m.avg(no class)   359\t 179\t 383\t 538\t 742\t66.7%\t48.4%\t56.1%\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "v9IJjNoBI--j"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "MUDLab6-DDI_LSTMcontext.ipynb",
      "provenance": [],
      "mount_file_id": "17zNQ3s92vBtrxhZlW5c1A5XXueSfft1D",
      "authorship_tag": "ABX9TyMh3I2MMbawD8/I4bOfn/+x",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}