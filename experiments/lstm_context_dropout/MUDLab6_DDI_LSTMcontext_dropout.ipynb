{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KatBCN/MUD-Lab-DDI-NN/blob/main/experiments/lstm_context_dropout/MUDLab6_DDI_LSTMcontext_dropout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MUD Lab Session 6 - DDI Using Neural Networks\n",
        "\n",
        "Change Runtime Type to GPU before running."
      ],
      "metadata": {
        "id": "tpJid7hPK-gG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mX7VXCjWfKeQ"
      },
      "outputs": [],
      "source": [
        "#!pip install tensorflow_addons"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "# mount drive\n",
        "drive.mount('/content/drive/')\n",
        "%cd /content/drive/MyDrive/MUD-Lab-6\n",
        "%ls "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zxFamfkLG7i",
        "outputId": "5ef20745-c80b-4d03-eb9d-156ad46e77ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "/content/drive/MyDrive/MUD-Lab-6\n",
            "codemaps_lw_pos.py  \u001b[0m\u001b[01;34mdata\u001b[0m/       devel.out     \u001b[01;34mExperiments\u001b[0m/  \u001b[01;34m__pycache__\u001b[0m/\n",
            "codemaps_pos.py     dataset.py  devel.stats   \u001b[01;34mmodel\u001b[0m/        \u001b[01;34mutil\u001b[0m/\n",
            "codemaps.py         deptree.py  evaluator.py  model.idx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BowuMcDbdnMe"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import time\n",
        "sys.path.insert(1, \"/content/drive/MyDrive/MUD-Lab-6\")\n",
        "\n",
        "import random\n",
        "from contextlib import redirect_stdout\n",
        "\n",
        "from tensorflow.keras import regularizers, Input\n",
        "from tensorflow.keras.models import Model,load_model\n",
        "from tensorflow.keras.layers import Embedding, Dense, Dropout, Conv1D, MaxPool1D, Reshape, Concatenate, Flatten, Bidirectional, LSTM\n",
        "\n",
        "\n",
        "from deptree import *\n",
        "from dataset import *\n",
        "from codemaps import *\n",
        "import evaluator\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(23)\n",
        "random.seed(23)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment\n",
        "\n",
        "\n",
        "- Conv1D: filters=50 (baseline was 30)\n",
        "- Changing the vector that flattens the convolutional output by concatenating the vectors to an LSTM context vector\n",
        "  - dropout=0.1 in LSTM layer\n",
        "\n",
        "\n",
        "\n",
        "[Keras Reference: LSTM layer](https://keras.io/api/layers/recurrent_layers/lstm/)"
      ],
      "metadata": {
        "id": "U6ZPdbkcLYUt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVs3b3mChb01"
      },
      "outputs": [],
      "source": [
        "#this is building the neural network.\n",
        "def build_network(idx) :\n",
        "\n",
        "   # sizes\n",
        "   n_words = codes.get_n_words() # number of words in vocabulary\n",
        "   max_len = codes.maxlen\n",
        "   n_labels = codes.get_n_labels()\n",
        "\n",
        "   # word input layer & embeddings\n",
        "   inptW = Input(shape=(max_len,)) # shape of input: all vectors need to be the same length.\n",
        "   embW = Embedding(input_dim=n_words, output_dim=100, # output_dim is the hyperparameter that can be tuned.\n",
        "                      input_length=max_len, mask_zero=False)(inptW)  \n",
        "\n",
        "  # number of filters it the output dimension which can be tuned.\n",
        "   conv = Conv1D(filters=50, kernel_size=2, strides=1, activation='relu', padding='same')(embW) # kernel size is the context of words, depending on the stride, you can have overlapping.\n",
        "   # flat= Flatten()(conv) # concatenating the vectors one after the other - we can change this architecture.\n",
        "   lstm = LSTM(units=50,dropout=0.1)(conv) # context vector - adding dropout to regularize.\n",
        "\n",
        "   # we need a single vector\n",
        "   # out = Dense(n_labels, activation='softmax')(flat)\n",
        "   out = Dense(n_labels, activation='softmax')(lstm)\n",
        "\n",
        "   model = Model(inptW, out)\n",
        "   model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "   return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZuXt4Sf1ifKc"
      },
      "outputs": [],
      "source": [
        "# directory with files to process\n",
        "trainfile = \"/content/drive/MyDrive/MUD-Lab-6/data/train.pck\"\n",
        "validationfile = \"/content/drive/MyDrive/MUD-Lab-6/data/devel.pck\"\n",
        "modelname = \"model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tN5elfAkhjqf",
        "outputId": "db176f28-99af-4195-9dfa-4e97bd5f74c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 150)]             0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 150, 100)          455300    \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 150, 50)           10050     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 50)                20200     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 5)                 255       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 485,805\n",
            "Trainable params: 485,805\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "724/724 [==============================] - 12s 11ms/step - loss: 0.6105 - accuracy: 0.8517 - val_loss: 0.6319 - val_accuracy: 0.8393\n",
            "Epoch 2/10\n",
            "724/724 [==============================] - 7s 10ms/step - loss: 0.5931 - accuracy: 0.8532 - val_loss: 0.6316 - val_accuracy: 0.8393\n",
            "Epoch 3/10\n",
            "724/724 [==============================] - 7s 10ms/step - loss: 0.5604 - accuracy: 0.8532 - val_loss: 0.5678 - val_accuracy: 0.8393\n",
            "Epoch 4/10\n",
            "724/724 [==============================] - 7s 10ms/step - loss: 0.5127 - accuracy: 0.8532 - val_loss: 0.5211 - val_accuracy: 0.8393\n",
            "Epoch 5/10\n",
            "724/724 [==============================] - 7s 10ms/step - loss: 0.4306 - accuracy: 0.8608 - val_loss: 0.5341 - val_accuracy: 0.8425\n",
            "Epoch 6/10\n",
            "724/724 [==============================] - 7s 10ms/step - loss: 0.3492 - accuracy: 0.8781 - val_loss: 0.5405 - val_accuracy: 0.8499\n",
            "Epoch 7/10\n",
            "724/724 [==============================] - 7s 10ms/step - loss: 0.3009 - accuracy: 0.8915 - val_loss: 0.5310 - val_accuracy: 0.8399\n",
            "Epoch 8/10\n",
            "724/724 [==============================] - 7s 10ms/step - loss: 0.2689 - accuracy: 0.8992 - val_loss: 0.5325 - val_accuracy: 0.8206\n",
            "Epoch 9/10\n",
            "724/724 [==============================] - 7s 10ms/step - loss: 0.2442 - accuracy: 0.9085 - val_loss: 0.5170 - val_accuracy: 0.8295\n",
            "Epoch 10/10\n",
            "724/724 [==============================] - 7s 10ms/step - loss: 0.2198 - accuracy: 0.9202 - val_loss: 0.5094 - val_accuracy: 0.8351\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: model/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: model/assets\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1a9e1a20d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ],
      "source": [
        "## --------- MAIN PROGRAM ----------- \n",
        "## --\n",
        "## -- Usage:  train.py ../data/Train ../data/Devel  modelname\n",
        "## --\n",
        "\n",
        "## --------- MAIN PROGRAM ----------- \n",
        "## --\n",
        "## -- Usage:  train.py ../data/Train ../data/Devel  modelname\n",
        "## --\n",
        "\n",
        "\n",
        "# directory with files to process\n",
        "# trainfile = sys.argv[1]\n",
        "# validationfile = sys.argv[2]\n",
        "# modelname = sys.argv[3]\n",
        "\n",
        "# load train and validation data\n",
        "traindata = Dataset(trainfile)\n",
        "valdata = Dataset(validationfile)\n",
        "\n",
        "# create indexes from training data\n",
        "max_len = 150\n",
        "suf_len = 5\n",
        "codes = Codemaps(traindata, max_len)\n",
        "\n",
        "# build network\n",
        "model = build_network(codes)\n",
        "with redirect_stdout(sys.stderr) :\n",
        "   model.summary()\n",
        "\n",
        "# encode datasets\n",
        "Xt = codes.encode_words(traindata)\n",
        "Yt = codes.encode_labels(traindata)\n",
        "Xv = codes.encode_words(valdata)\n",
        "Yv = codes.encode_labels(valdata)\n",
        "\n",
        "#track time to train model\n",
        "start_time = time.time()\n",
        "# train model\n",
        "with redirect_stdout(sys.stderr) :\n",
        "   model.fit(Xt, Yt, batch_size=32, epochs=10, validation_data=(Xv,Yv), verbose=1) # changing number of epochs fewer to prevent overfitting.\n",
        "\n",
        "train_time = time.time() - start_time\n",
        "\n",
        "# save model and indexs\n",
        "model.save(modelname)\n",
        "codes.save(modelname)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Amount of time to train the model is:\", train_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMcRL_3gOSBs",
        "outputId": "0118e2d2-b221-4abf-b050-d1477c6208b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Amount of time to train the model is: 83.53653001785278\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fname = \"/content/drive/MyDrive/MUD-Lab-6/model\"\n",
        "datafile = \"/content/drive/MyDrive/MUD-Lab-6/data/devel.pck\"\n",
        "outfile = \"devel.out\""
      ],
      "metadata": {
        "id": "jq2RfnjNpFmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code chunk is from predict.py"
      ],
      "metadata": {
        "id": "SXlLXmYxoR7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# predict.py\n",
        "\n",
        "## --------- Entity extractor ----------- \n",
        "## -- Extract drug entities from given text and return them as\n",
        "## -- a list of dictionaries with keys \"offset\", \"text\", and \"type\"\n",
        "\n",
        "def output_interactions(data, preds, outfile) :\n",
        "\n",
        "   #print(testdata[0])\n",
        "   outf = open(outfile, 'w')\n",
        "   for exmp,tag in zip(data.sentences(),preds) :\n",
        "      sid = exmp['sid']\n",
        "      e1 = exmp['e1']\n",
        "      e2 = exmp['e2']\n",
        "      if tag!='null' :\n",
        "         print(sid, e1, e2, tag, sep=\"|\", file=outf)\n",
        "            \n",
        "   outf.close()\n",
        "\n",
        "   \n",
        "## --------- MAIN PROGRAM ----------- \n",
        "## --\n",
        "## -- Usage:  baseline-NER.py target-dir\n",
        "## --\n",
        "## -- Extracts Drug NE from all XML files in target-dir\n",
        "## --\n",
        "\n",
        "#fname = sys.argv[1]\n",
        "#datafile = sys.argv[2]\n",
        "#outfile = sys.argv[3]\n",
        "\n",
        "model = load_model(fname)\n",
        "codes = Codemaps(fname)\n",
        "\n",
        "testdata = Dataset(datafile)\n",
        "X = codes.encode_words(testdata)\n",
        "\n",
        "Y = model.predict(X)\n",
        "Y = [codes.idx2label(np.argmax(s)) for s in Y]\n",
        "\n",
        "# extract relations\n",
        "output_interactions(testdata, Y, outfile)\n"
      ],
      "metadata": {
        "id": "vHXEGZNkn3Rt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task = \"DDI\"\n",
        "golddir = \"/content/drive/MyDrive/MUD-Lab-6/data/devel\"\n",
        "outfile = \"devel.out\""
      ],
      "metadata": {
        "id": "Vb9e-qHwrpaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code chunk is from evaluator.py and creates file devel.stats"
      ],
      "metadata": {
        "id": "JOIWKxLpoisY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluator.py\n",
        "\n",
        "#! /usr/bin/python3\n",
        "\n",
        "import sys\n",
        "from os import listdir\n",
        "\n",
        "from xml.dom.minidom import parse\n",
        "\n",
        "## --\n",
        "## -- auxliary to insert an instance in given instance_set\n",
        "## --\n",
        "\n",
        "def add_instance(instance_set, einfo, etype) :\n",
        "    instance_set[\"CLASS\"].add(einfo+\"|\"+etype)\n",
        "    instance_set[\"NOCLASS\"].add(einfo)\n",
        "    if etype not in instance_set : instance_set[etype] = set([])\n",
        "    instance_set[etype].add(einfo)\n",
        "\n",
        "    \n",
        "## --\n",
        "## -- Load entities from XML files in given golddir\n",
        "## --\n",
        "\n",
        "\"\"\"\n",
        "def load_gold_NER(golddir) :\n",
        "    entities = { \"CLASS\" : set([]), \"NOCLASS\" : set([]) }\n",
        "\n",
        "    # process each file in directory\n",
        "    for f in listdir(golddir) :\n",
        "\n",
        "        # parse XML file, obtaining a DOM tree\n",
        "        tree = parse(golddir+\"/\"+f)\n",
        "\n",
        "        # process each sentence in the file\n",
        "        sentences = tree.getElementsByTagName(\"sentence\")\n",
        "        for s in sentences :\n",
        "            sid = s.attributes[\"id\"].value   # get sentence id\n",
        "            \n",
        "            # load sentence entities\n",
        "            ents = s.getElementsByTagName(\"entity\")\n",
        "            for e in ents :\n",
        "                einfo = sid + \"|\" + e.attributes[\"charOffset\"].value  + \"|\" + e.attributes[\"text\"].value\n",
        "                etype = e.attributes[\"type\"].value\n",
        "                add_instance(entities, einfo, etype)\n",
        "            \n",
        "    return entities\n",
        "\"\"\"\n",
        "## --\n",
        "## -- Load relations from XML files in given golddir\n",
        "## --\n",
        "\n",
        "def load_gold_DDI(golddir) :\n",
        "    relations = { \"CLASS\" : set([]), \"NOCLASS\" : set([]) }\n",
        "\n",
        "    # process each file in directory\n",
        "    for f in listdir(golddir) :\n",
        "\n",
        "        # parse XML file, obtaining a DOM tree\n",
        "        tree = parse(golddir+\"/\"+f)\n",
        "\n",
        "        # process each sentence in the file\n",
        "        sentences = tree.getElementsByTagName(\"sentence\")\n",
        "        for s in sentences :\n",
        "            sid = s.attributes[\"id\"].value   # get sentence id\n",
        "        \n",
        "            # load \"pairs\"  in the sentence, keep those with ddi=true\n",
        "            pairs = s.getElementsByTagName(\"pair\")\n",
        "            for p in pairs:\n",
        "                id_e1 = p.attributes[\"e1\"].value\n",
        "                id_e2 = p.attributes[\"e2\"].value\n",
        "                ddi = p.attributes[\"ddi\"].value\n",
        "\n",
        "                if (ddi == \"true\") :\n",
        "                    rtype = p.attributes[\"type\"].value\n",
        "                    rinfo = sid + \"|\" + id_e1 + \"|\" +  id_e2\n",
        "                    add_instance(relations, rinfo, rtype)\n",
        "\n",
        "    return relations\n",
        "\n",
        "\n",
        "## --\n",
        "## -- Load entities/relations from given system output file\n",
        "## --\n",
        "\n",
        "def load_predicted(task, outfile) :\n",
        "    predicted = { \"CLASS\" : set([]), \"NOCLASS\" : set([]) }\n",
        "    outf = open(outfile,\"r\")\n",
        "    for line in outf.readlines() :\n",
        "        line = line.strip()\n",
        "        if line in predicted[\"CLASS\"] :\n",
        "            print(\"Ignoring duplicated entity in system predictions file: \"+line)\n",
        "            continue\n",
        "\n",
        "        etype = line.split(\"|\")[-1]\n",
        "        einfo = \"|\".join(line.split(\"|\")[:-1])\n",
        "        add_instance(predicted, einfo, etype)\n",
        "        outf.close()\n",
        "        \n",
        "    return predicted\n",
        "    \n",
        "\n",
        "\n",
        "## --\n",
        "## -- Compare given sets and compute tp,fp,fn,P,R,F1\n",
        "## --\n",
        "\n",
        "def statistics(gold,predicted,kind) :\n",
        "    tp = 0\n",
        "    fp = 0\n",
        "    nexp = len(gold[kind])\n",
        "    if kind in predicted:\n",
        "        npred = len(predicted[kind])\n",
        "        for p in predicted[kind] :\n",
        "            if p in gold[kind] : tp += 1\n",
        "            else : fp += 1\n",
        "\n",
        "        fn = 0\n",
        "        for p in gold[kind] :\n",
        "            if p not in predicted[kind] : fn += 1\n",
        "\n",
        "    else :\n",
        "        npred = 0\n",
        "        fn = nexp\n",
        "\n",
        "    P = tp/npred if npred!=0 else 0\n",
        "    R = tp/nexp if nexp!=0 else 0    \n",
        "    F1 = 2*P*R/(P+R) if P+R!=0 else 0\n",
        "\n",
        "    return tp,fp,fn,npred,nexp,P,R,F1\n",
        "\n",
        "## --\n",
        "## -- Compute and print statistics table\n",
        "## --\n",
        "\n",
        "def row(txt) :\n",
        "   return txt + ' '*(17-len(txt))\n",
        "\n",
        "\n",
        "def print_statistics(gold,predicted, statfile) :\n",
        "    print(row(\"\")+\"  tp\\t  fp\\t  fn\\t#pred\\t#exp\\tP\\tR\\tF1\", file=statfile)\n",
        "    print(\"------------------------------------------------------------------------------\", file=statfile)\n",
        "    (nk,sP,sR,sF1) = (0,0,0,0)\n",
        "    for kind in sorted(gold) :\n",
        "        if kind==\"CLASS\" or kind==\"NOCLASS\" : continue\n",
        "        (tp,fp,fn,npred,nexp,P,R,F1) = statistics(gold, predicted, kind)\n",
        "        print(row(kind)+\"{:>4}\\t{:>4}\\t{:>4}\\t{:>4}\\t{:>4}\\t{:2.1%}\\t{:2.1%}\\t{:2.1%}\".format(tp,fp,fn,npred,nexp, P, R, F1), file=statfile)\n",
        "        (nk,sP,sR,sF1) = (nk+1, sP+P, sR+R, sF1+F1)\n",
        "\n",
        "    (sP, sR, sF1) = (sP/nk, sR/nk, sF1/nk)\n",
        "    print(\"------------------------------------------------------------------------------\", file=statfile)\n",
        "    print(row(\"M.avg\")+\"-\\t-\\t-\\t-\\t-\\t{:2.1%}\\t{:2.1%}\\t{:2.1%}\".format(sP, sR, sF1), file=statfile)\n",
        "\n",
        "    print(\"------------------------------------------------------------------------------\", file=statfile)\n",
        "    (tp,fp,fn,npred,nexp,P,R,F1) = statistics(gold, predicted, \"CLASS\")\n",
        "    print(row(\"m.avg\")+\"{:>4}\\t{:>4}\\t{:>4}\\t{:>4}\\t{:>4}\\t{:2.1%}\\t{:2.1%}\\t{:2.1%}\".format(tp,fp,fn,npred,nexp, P, R, F1), file=statfile)                        \n",
        "    (tp,fp,fn,npred,nexp,P,R,F1) = statistics(gold, predicted, \"NOCLASS\")\n",
        "    print(row(\"m.avg(no class)\")+\"{:>4}\\t{:>4}\\t{:>4}\\t{:>4}\\t{:>4}\\t{:2.1%}\\t{:2.1%}\\t{:2.1%}\".format(tp,fp,fn,npred,nexp, P, R, F1), file=statfile)               \n",
        "\n",
        "## --\n",
        "## -- Evaluates results in outfile comparing them with gold standard in golddir.\n",
        "## -- 'task' is either NER or DDI\n",
        "## -- This function can be called from any program requesting evaluation.\n",
        "## --\n",
        " \n",
        "def evaluate(task, golddir, outfile):\n",
        "\n",
        "    if task==\"NER\" :\n",
        "        # get set of expected entities in the whole golddir\n",
        "        gold = load_gold_NER(golddir)\n",
        "    elif task == \"DDI\" :\n",
        "        # get set of expected relations in the whole golddir\n",
        "        gold = load_gold_DDI(golddir)\n",
        "    else :\n",
        "        print (\"Invalid task '\"+task+\"'. Please specify 'NER' or 'DDI'.\")        \n",
        "\n",
        "\n",
        "    # Load entities/relations predicted by the system\n",
        "    predicted = load_predicted(task, outfile)\n",
        "\n",
        "    # compare both sets and compute statistics\n",
        "    statfile = open(\"devel.stats\", 'w')\n",
        "    print_statistics(gold,predicted, statfile)\n",
        "    statfile.close()\n",
        "\n",
        "         \n",
        "        \n",
        "## --\n",
        "## -- Usage as standalone program:  evaluator.py (NER|DDI) golddir outfile\n",
        "## --\n",
        "## -- Evaluates results in outfile comparing them with gold standard in golddir\n",
        "## --\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    #if len(sys.argv) != 4 :\n",
        "     #   print(\"\\n  Usage: evaluator.py (NER|DDI) golddir outfile\\n\")\n",
        "      #  exit()\n",
        "        \n",
        "    task = \"DDI\"\n",
        "    golddir = \"/content/drive/MyDrive/MUD-Lab-6/data/devel\"\n",
        "    outfile = \"devel.out\"\n",
        "\n",
        "    evaluate(task, golddir, outfile)\n"
      ],
      "metadata": {
        "id": "hOK3PizbohBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results with LSTM dropout=0.1\n",
        "\n",
        "Compared to a dropout of 0.0, increasing the dropout to 0.1 decreased the M.avg F1 score from 37.8% to 32.2%. The most notable drop was the F1 score for advise which reduced from 43.0% to 1.3%. \n",
        "\n",
        "The other categories had some improvement with true positives.\n",
        "\n",
        "devel.stats:"
      ],
      "metadata": {
        "id": "hLt4nn9CRpO1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "                   tp\t  fp\t  fn\t#pred\t#exp\tP\tR\tF1\n",
        "------------------------------------------------------------------------------\n",
        "advise              1\t  16\t 140\t  17\t 141\t5.9%\t0.7%\t1.3%\n",
        "effect            190\t 306\t 122\t 496\t 312\t38.3%\t60.9%\t47.0%\n",
        "int                13\t   2\t  15\t  15\t  28\t86.7%\t46.4%\t60.5%\n",
        "mechanism          45\t 140\t 216\t 185\t 261\t24.3%\t17.2%\t20.2%\n",
        "------------------------------------------------------------------------------\n",
        "M.avg            -\t-\t-\t-\t-\t38.8%\t31.3%\t32.2%\n",
        "------------------------------------------------------------------------------\n",
        "m.avg             249\t 464\t 493\t 713\t 742\t34.9%\t33.6%\t34.2%\n",
        "m.avg(no class)   445\t 268\t 297\t 713\t 742\t62.4%\t60.0%\t61.2%\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "BZMWmQBjRgon"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "MUDLab6-DDI_LSTMcontext_dropout.ipynb",
      "provenance": [],
      "mount_file_id": "17zNQ3s92vBtrxhZlW5c1A5XXueSfft1D",
      "authorship_tag": "ABX9TyMKS5fCK7Zf1/EFtydo2jd9",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}