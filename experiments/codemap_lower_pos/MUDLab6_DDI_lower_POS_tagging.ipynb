{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KatBCN/MUD-Lab-DDI-NN/blob/main/experiments/codemap_lower_pos/MUDLab6_DDI_lower_POS_tagging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change Runtime Type to GPU before running."
      ],
      "metadata": {
        "id": "9c_sPkDRfprM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mX7VXCjWfKeQ"
      },
      "outputs": [],
      "source": [
        "#!pip install tensorflow_addons"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "# mount drive\n",
        "drive.mount('/content/drive/')\n",
        "%cd /content/drive/MyDrive/MUD-Lab-6\n",
        "%ls "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyhbE3RYqcUV",
        "outputId": "629a8df5-93ef-4098-911b-c2eb8249262e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "/content/drive/MyDrive/MUD-Lab-6\n",
            "codemaps_lw_pos.py  \u001b[0m\u001b[01;34mdata\u001b[0m/       devel.out     \u001b[01;34mExperiments\u001b[0m/  \u001b[01;34m__pycache__\u001b[0m/\n",
            "codemaps_pos.py     dataset.py  devel.stats   \u001b[01;34mmodel\u001b[0m/        \u001b[01;34mutil\u001b[0m/\n",
            "codemaps.py         deptree.py  evaluator.py  model.idx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BowuMcDbdnMe"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import time\n",
        "sys.path.insert(1, \"/content/drive/MyDrive/MUD-Lab-6\")\n",
        "\n",
        "import random\n",
        "from contextlib import redirect_stdout\n",
        "\n",
        "from tensorflow.keras import regularizers, Input\n",
        "from tensorflow.keras.models import Model,load_model\n",
        "from tensorflow.keras.layers import Embedding, Dense, Dropout, Conv1D, MaxPool1D, Reshape, Concatenate, Flatten, Bidirectional, LSTM\n",
        "\n",
        "\n",
        "from deptree import *\n",
        "from dataset import *\n",
        "# codemaps for lowercase words and pos tags\n",
        "from codemaps_lw_pos import *\n",
        "import evaluator\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(23)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment with POS tagging & changing words to lower case\n",
        "\n",
        "- Uses 150 as the out output_dim for the input word embedding instead of 100.\n",
        "\n",
        "This model uses code provided by the course."
      ],
      "metadata": {
        "id": "pa62EjZFfCo1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EVs3b3mChb01"
      },
      "outputs": [],
      "source": [
        "#this is building the neural network.\n",
        "def build_network(idx) :\n",
        "\n",
        "   # sizes\n",
        "   n_words = codes.get_n_lc_words() # number of lowercase words in vocabulary\n",
        "   n_pos = codes.get_n_pos() # number of pos tags\n",
        "   max_len = codes.maxlen\n",
        "   n_labels = codes.get_n_labels()\n",
        "\n",
        "   # word input layer & embeddings\n",
        "   inptW = Input(shape=(max_len,)) # shape of input: all vectors need to be the same length.\n",
        "   embW = Embedding(input_dim=n_words, output_dim=150, # output_dim is the hyperparameter that can be tuned.\n",
        "                      input_length=max_len, mask_zero=False)(inptW)  \n",
        "\n",
        "  # POS input layer & embeddings\n",
        "   inptPOS = Input(shape=(max_len,)) # shape of input: all vectors need to be the same length.\n",
        "   embPOS = Embedding(input_dim=n_pos, output_dim=100, # output_dim is the hyperparameter that can be tuned.\n",
        "                      input_length=max_len, mask_zero=False)(inptPOS)\n",
        "\n",
        "   conc = Concatenate()([embW, embPOS])\n",
        "\n",
        "  # number of filters it the output dimension which can be tuned.\n",
        "   conv = Conv1D(filters=30, kernel_size=2, strides=1, activation='relu', padding='same')(conc) # kernel size is the context of words, depending on the stride, you can have overlapping.\n",
        "   flat= Flatten()(conv) # concatenating the vectors one after the other - we can change this architecture.\n",
        "\n",
        "   # we need a single vector\n",
        "   \n",
        "   out = Dense(n_labels, activation='softmax')(flat)\n",
        "\n",
        "   model = Model([inptW, inptPOS], out)\n",
        "   model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "   return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZuXt4Sf1ifKc"
      },
      "outputs": [],
      "source": [
        "# directory with files to process\n",
        "trainfile = \"/content/drive/MyDrive/MUD-Lab-6/data/train.pck\"\n",
        "validationfile = \"/content/drive/MyDrive/MUD-Lab-6/data/devel.pck\"\n",
        "modelname = \"model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tN5elfAkhjqf",
        "outputId": "921a191f-7dd7-4cdc-8d09-e4fdce5d0a1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 150)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 150)]        0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 150, 150)     597450      ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, 150, 100)     5100        ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 150, 250)     0           ['embedding[0][0]',              \n",
            "                                                                  'embedding_1[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 150, 30)      15030       ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 4500)         0           ['conv1d[0][0]']                 \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 5)            22505       ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 640,085\n",
            "Trainable params: 640,085\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/10\n",
            "724/724 [==============================] - 16s 6ms/step - loss: 0.4578 - accuracy: 0.8593 - val_loss: 0.4406 - val_accuracy: 0.8596\n",
            "Epoch 2/10\n",
            "724/724 [==============================] - 3s 5ms/step - loss: 0.2676 - accuracy: 0.9014 - val_loss: 0.4090 - val_accuracy: 0.8655\n",
            "Epoch 3/10\n",
            "724/724 [==============================] - 3s 5ms/step - loss: 0.2085 - accuracy: 0.9217 - val_loss: 0.4227 - val_accuracy: 0.8718\n",
            "Epoch 4/10\n",
            "724/724 [==============================] - 3s 5ms/step - loss: 0.1808 - accuracy: 0.9327 - val_loss: 0.4155 - val_accuracy: 0.8698\n",
            "Epoch 5/10\n",
            "724/724 [==============================] - 4s 5ms/step - loss: 0.1625 - accuracy: 0.9390 - val_loss: 0.4273 - val_accuracy: 0.8705\n",
            "Epoch 6/10\n",
            "724/724 [==============================] - 3s 5ms/step - loss: 0.1485 - accuracy: 0.9458 - val_loss: 0.4560 - val_accuracy: 0.8748\n",
            "Epoch 7/10\n",
            "724/724 [==============================] - 3s 5ms/step - loss: 0.1365 - accuracy: 0.9488 - val_loss: 0.4450 - val_accuracy: 0.8635\n",
            "Epoch 8/10\n",
            "724/724 [==============================] - 3s 5ms/step - loss: 0.1255 - accuracy: 0.9539 - val_loss: 0.4578 - val_accuracy: 0.8637\n",
            "Epoch 9/10\n",
            "724/724 [==============================] - 3s 5ms/step - loss: 0.1190 - accuracy: 0.9566 - val_loss: 0.4709 - val_accuracy: 0.8694\n",
            "Epoch 10/10\n",
            "724/724 [==============================] - 3s 5ms/step - loss: 0.1093 - accuracy: 0.9600 - val_loss: 0.5265 - val_accuracy: 0.8724\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: model/assets\n"
          ]
        }
      ],
      "source": [
        "## --------- MAIN PROGRAM ----------- \n",
        "## --\n",
        "## -- Usage:  train.py ../data/Train ../data/Devel  modelname\n",
        "## --\n",
        "\n",
        "## --------- MAIN PROGRAM ----------- \n",
        "## --\n",
        "## -- Usage:  train.py ../data/Train ../data/Devel  modelname\n",
        "## --\n",
        "\n",
        "\n",
        "# directory with files to process\n",
        "# trainfile = sys.argv[1]\n",
        "# validationfile = sys.argv[2]\n",
        "# modelname = sys.argv[3]\n",
        "\n",
        "# load train and validation data\n",
        "traindata = Dataset(trainfile)\n",
        "valdata = Dataset(validationfile)\n",
        "\n",
        "# create indexes from training data\n",
        "max_len = 150\n",
        "suf_len = 5\n",
        "codes = Codemaps(traindata, max_len)\n",
        "\n",
        "# build network\n",
        "model = build_network(codes)\n",
        "with redirect_stdout(sys.stderr) :\n",
        "   model.summary()\n",
        "\n",
        "# encode datasets\n",
        "Xt = codes.encode_words(traindata)\n",
        "Yt = codes.encode_labels(traindata)\n",
        "Xv = codes.encode_words(valdata)\n",
        "Yv = codes.encode_labels(valdata)\n",
        "\n",
        "#track time to train model\n",
        "start_time = time.time()\n",
        "\n",
        "# train model\n",
        "with redirect_stdout(sys.stderr) :\n",
        "   model.fit(Xt, Yt, batch_size=32, epochs=10, validation_data=(Xv,Yv), verbose=1) # add class_weight here for experiments.\n",
        "\n",
        "train_time = time.time() - start_time\n",
        "\n",
        "# save model and indexs\n",
        "model.save(modelname)\n",
        "codes.save(modelname)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Amount of time to train the model is:\", train_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pe33L-95i08y",
        "outputId": "f93322ef-1970-4029-d119-0f18f4de6f83"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Amount of time to train the model is: 82.60335373878479\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fname = \"/content/drive/MyDrive/MUD-Lab-6/model\"\n",
        "datafile = \"/content/drive/MyDrive/MUD-Lab-6/data/devel.pck\"\n",
        "outfile = \"devel.out\""
      ],
      "metadata": {
        "id": "jq2RfnjNpFmc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code chunk is from predict.py"
      ],
      "metadata": {
        "id": "SXlLXmYxoR7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# predict.py\n",
        "\n",
        "## --------- Entity extractor ----------- \n",
        "## -- Extract drug entities from given text and return them as\n",
        "## -- a list of dictionaries with keys \"offset\", \"text\", and \"type\"\n",
        "\n",
        "def output_interactions(data, preds, outfile) :\n",
        "\n",
        "   #print(testdata[0])\n",
        "   outf = open(outfile, 'w')\n",
        "   for exmp,tag in zip(data.sentences(),preds) :\n",
        "      sid = exmp['sid']\n",
        "      e1 = exmp['e1']\n",
        "      e2 = exmp['e2']\n",
        "      if tag!='null' :\n",
        "         print(sid, e1, e2, tag, sep=\"|\", file=outf)\n",
        "            \n",
        "   outf.close()\n",
        "\n",
        "   \n",
        "## --------- MAIN PROGRAM ----------- \n",
        "## --\n",
        "## -- Usage:  baseline-NER.py target-dir\n",
        "## --\n",
        "## -- Extracts Drug NE from all XML files in target-dir\n",
        "## --\n",
        "\n",
        "#fname = sys.argv[1]\n",
        "#datafile = sys.argv[2]\n",
        "#outfile = sys.argv[3]\n",
        "\n",
        "model = load_model(fname)\n",
        "codes = Codemaps(fname)\n",
        "\n",
        "testdata = Dataset(datafile)\n",
        "X = codes.encode_words(testdata)\n",
        "\n",
        "Y = model.predict(X)\n",
        "Y = [codes.idx2label(np.argmax(s)) for s in Y]\n",
        "\n",
        "# extract relations\n",
        "output_interactions(testdata, Y, outfile)\n"
      ],
      "metadata": {
        "id": "vHXEGZNkn3Rt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task = \"DDI\"\n",
        "golddir = \"/content/drive/MyDrive/MUD-Lab-6/data/devel\"\n",
        "outfile = \"devel.out\""
      ],
      "metadata": {
        "id": "Vb9e-qHwrpaC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code chunk is from evaluator.py and creates file devel.stats"
      ],
      "metadata": {
        "id": "JOIWKxLpoisY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluator.py\n",
        "\n",
        "#! /usr/bin/python3\n",
        "\n",
        "import sys\n",
        "from os import listdir\n",
        "\n",
        "from xml.dom.minidom import parse\n",
        "\n",
        "## --\n",
        "## -- auxliary to insert an instance in given instance_set\n",
        "## --\n",
        "\n",
        "def add_instance(instance_set, einfo, etype) :\n",
        "    instance_set[\"CLASS\"].add(einfo+\"|\"+etype)\n",
        "    instance_set[\"NOCLASS\"].add(einfo)\n",
        "    if etype not in instance_set : instance_set[etype] = set([])\n",
        "    instance_set[etype].add(einfo)\n",
        "\n",
        "    \n",
        "## --\n",
        "## -- Load entities from XML files in given golddir\n",
        "## --\n",
        "\n",
        "\"\"\"\n",
        "def load_gold_NER(golddir) :\n",
        "    entities = { \"CLASS\" : set([]), \"NOCLASS\" : set([]) }\n",
        "\n",
        "    # process each file in directory\n",
        "    for f in listdir(golddir) :\n",
        "\n",
        "        # parse XML file, obtaining a DOM tree\n",
        "        tree = parse(golddir+\"/\"+f)\n",
        "\n",
        "        # process each sentence in the file\n",
        "        sentences = tree.getElementsByTagName(\"sentence\")\n",
        "        for s in sentences :\n",
        "            sid = s.attributes[\"id\"].value   # get sentence id\n",
        "            \n",
        "            # load sentence entities\n",
        "            ents = s.getElementsByTagName(\"entity\")\n",
        "            for e in ents :\n",
        "                einfo = sid + \"|\" + e.attributes[\"charOffset\"].value  + \"|\" + e.attributes[\"text\"].value\n",
        "                etype = e.attributes[\"type\"].value\n",
        "                add_instance(entities, einfo, etype)\n",
        "            \n",
        "    return entities\n",
        "\"\"\"\n",
        "## --\n",
        "## -- Load relations from XML files in given golddir\n",
        "## --\n",
        "\n",
        "def load_gold_DDI(golddir) :\n",
        "    relations = { \"CLASS\" : set([]), \"NOCLASS\" : set([]) }\n",
        "\n",
        "    # process each file in directory\n",
        "    for f in listdir(golddir) :\n",
        "\n",
        "        # parse XML file, obtaining a DOM tree\n",
        "        tree = parse(golddir+\"/\"+f)\n",
        "\n",
        "        # process each sentence in the file\n",
        "        sentences = tree.getElementsByTagName(\"sentence\")\n",
        "        for s in sentences :\n",
        "            sid = s.attributes[\"id\"].value   # get sentence id\n",
        "        \n",
        "            # load \"pairs\"  in the sentence, keep those with ddi=true\n",
        "            pairs = s.getElementsByTagName(\"pair\")\n",
        "            for p in pairs:\n",
        "                id_e1 = p.attributes[\"e1\"].value\n",
        "                id_e2 = p.attributes[\"e2\"].value\n",
        "                ddi = p.attributes[\"ddi\"].value\n",
        "\n",
        "                if (ddi == \"true\") :\n",
        "                    rtype = p.attributes[\"type\"].value\n",
        "                    rinfo = sid + \"|\" + id_e1 + \"|\" +  id_e2\n",
        "                    add_instance(relations, rinfo, rtype)\n",
        "\n",
        "    return relations\n",
        "\n",
        "\n",
        "## --\n",
        "## -- Load entities/relations from given system output file\n",
        "## --\n",
        "\n",
        "def load_predicted(task, outfile) :\n",
        "    predicted = { \"CLASS\" : set([]), \"NOCLASS\" : set([]) }\n",
        "    outf = open(outfile,\"r\")\n",
        "    for line in outf.readlines() :\n",
        "        line = line.strip()\n",
        "        if line in predicted[\"CLASS\"] :\n",
        "            print(\"Ignoring duplicated entity in system predictions file: \"+line)\n",
        "            continue\n",
        "\n",
        "        etype = line.split(\"|\")[-1]\n",
        "        einfo = \"|\".join(line.split(\"|\")[:-1])\n",
        "        add_instance(predicted, einfo, etype)\n",
        "        outf.close()\n",
        "        \n",
        "    return predicted\n",
        "    \n",
        "\n",
        "\n",
        "## --\n",
        "## -- Compare given sets and compute tp,fp,fn,P,R,F1\n",
        "## --\n",
        "\n",
        "def statistics(gold,predicted,kind) :\n",
        "    tp = 0\n",
        "    fp = 0\n",
        "    nexp = len(gold[kind])\n",
        "    if kind in predicted:\n",
        "        npred = len(predicted[kind])\n",
        "        for p in predicted[kind] :\n",
        "            if p in gold[kind] : tp += 1\n",
        "            else : fp += 1\n",
        "\n",
        "        fn = 0\n",
        "        for p in gold[kind] :\n",
        "            if p not in predicted[kind] : fn += 1\n",
        "\n",
        "    else :\n",
        "        npred = 0\n",
        "        fn = nexp\n",
        "\n",
        "    P = tp/npred if npred!=0 else 0\n",
        "    R = tp/nexp if nexp!=0 else 0    \n",
        "    F1 = 2*P*R/(P+R) if P+R!=0 else 0\n",
        "\n",
        "    return tp,fp,fn,npred,nexp,P,R,F1\n",
        "\n",
        "## --\n",
        "## -- Compute and print statistics table\n",
        "## --\n",
        "\n",
        "def row(txt) :\n",
        "   return txt + ' '*(17-len(txt))\n",
        "\n",
        "\n",
        "def print_statistics(gold,predicted, statfile) :\n",
        "    print(row(\"\")+\"  tp\\t  fp\\t  fn\\t#pred\\t#exp\\tP\\tR\\tF1\", file=statfile)\n",
        "    print(\"------------------------------------------------------------------------------\", file=statfile)\n",
        "    (nk,sP,sR,sF1) = (0,0,0,0)\n",
        "    for kind in sorted(gold) :\n",
        "        if kind==\"CLASS\" or kind==\"NOCLASS\" : continue\n",
        "        (tp,fp,fn,npred,nexp,P,R,F1) = statistics(gold, predicted, kind)\n",
        "        print(row(kind)+\"{:>4}\\t{:>4}\\t{:>4}\\t{:>4}\\t{:>4}\\t{:2.1%}\\t{:2.1%}\\t{:2.1%}\".format(tp,fp,fn,npred,nexp, P, R, F1), file=statfile)\n",
        "        (nk,sP,sR,sF1) = (nk+1, sP+P, sR+R, sF1+F1)\n",
        "\n",
        "    (sP, sR, sF1) = (sP/nk, sR/nk, sF1/nk)\n",
        "    print(\"------------------------------------------------------------------------------\", file=statfile)\n",
        "    print(row(\"M.avg\")+\"-\\t-\\t-\\t-\\t-\\t{:2.1%}\\t{:2.1%}\\t{:2.1%}\".format(sP, sR, sF1), file=statfile)\n",
        "\n",
        "    print(\"------------------------------------------------------------------------------\", file=statfile)\n",
        "    (tp,fp,fn,npred,nexp,P,R,F1) = statistics(gold, predicted, \"CLASS\")\n",
        "    print(row(\"m.avg\")+\"{:>4}\\t{:>4}\\t{:>4}\\t{:>4}\\t{:>4}\\t{:2.1%}\\t{:2.1%}\\t{:2.1%}\".format(tp,fp,fn,npred,nexp, P, R, F1), file=statfile)                        \n",
        "    (tp,fp,fn,npred,nexp,P,R,F1) = statistics(gold, predicted, \"NOCLASS\")\n",
        "    print(row(\"m.avg(no class)\")+\"{:>4}\\t{:>4}\\t{:>4}\\t{:>4}\\t{:>4}\\t{:2.1%}\\t{:2.1%}\\t{:2.1%}\".format(tp,fp,fn,npred,nexp, P, R, F1), file=statfile)               \n",
        "\n",
        "## --\n",
        "## -- Evaluates results in outfile comparing them with gold standard in golddir.\n",
        "## -- 'task' is either NER or DDI\n",
        "## -- This function can be called from any program requesting evaluation.\n",
        "## --\n",
        " \n",
        "def evaluate(task, golddir, outfile):\n",
        "\n",
        "    if task==\"NER\" :\n",
        "        # get set of expected entities in the whole golddir\n",
        "        gold = load_gold_NER(golddir)\n",
        "    elif task == \"DDI\" :\n",
        "        # get set of expected relations in the whole golddir\n",
        "        gold = load_gold_DDI(golddir)\n",
        "    else :\n",
        "        print (\"Invalid task '\"+task+\"'. Please specify 'NER' or 'DDI'.\")        \n",
        "\n",
        "\n",
        "    # Load entities/relations predicted by the system\n",
        "    predicted = load_predicted(task, outfile)\n",
        "\n",
        "    # compare both sets and compute statistics\n",
        "    statfile = open(\"devel.stats\", 'w')\n",
        "    print_statistics(gold,predicted, statfile)\n",
        "    statfile.close()\n",
        "\n",
        "         \n",
        "        \n",
        "## --\n",
        "## -- Usage as standalone program:  evaluator.py (NER|DDI) golddir outfile\n",
        "## --\n",
        "## -- Evaluates results in outfile comparing them with gold standard in golddir\n",
        "## --\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    #if len(sys.argv) != 4 :\n",
        "     #   print(\"\\n  Usage: evaluator.py (NER|DDI) golddir outfile\\n\")\n",
        "      #  exit()\n",
        "        \n",
        "    task = \"DDI\"\n",
        "    golddir = \"/content/drive/MyDrive/MUD-Lab-6/data/devel\"\n",
        "    outfile = \"devel.out\"\n",
        "\n",
        "    evaluate(task, golddir, outfile)\n"
      ],
      "metadata": {
        "id": "hOK3PizbohBR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results\n",
        "\n",
        "Replacing the words with lowercase and adding pos tags did not hurt the model very much, but it also didn't improve it.\n",
        "\n",
        "devel.stats:\n",
        "\n",
        "\n",
        "```\n",
        "                   tp\t  fp\t  fn\t#pred\t#exp\tP\tR\tF1\n",
        "------------------------------------------------------------------------------\n",
        "advise             96\t  61\t  45\t 157\t 141\t61.1%\t68.1%\t64.4%\n",
        "effect            153\t 113\t 159\t 266\t 312\t57.5%\t49.0%\t52.9%\n",
        "int                14\t  10\t  14\t  24\t  28\t58.3%\t50.0%\t53.8%\n",
        "mechanism          66\t  38\t 195\t 104\t 261\t63.5%\t25.3%\t36.2%\n",
        "------------------------------------------------------------------------------\n",
        "M.avg            -\t-\t-\t-\t-\t60.1%\t48.1%\t51.8%\n",
        "------------------------------------------------------------------------------\n",
        "m.avg             329\t 222\t 413\t 551\t 742\t59.7%\t44.3%\t50.9%\n",
        "m.avg(no class)   375\t 176\t 367\t 551\t 742\t68.1%\t50.5%\t58.0%\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "R8Wyu4x3gJLs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline Results:"
      ],
      "metadata": {
        "id": "paHfdeP9Jac1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "devel.stats:\n",
        "\n",
        "```\n",
        "                   tp\t  fp\t  fn\t#pred\t#exp\tP\tR\tF1\n",
        "------------------------------------------------------------------------------\n",
        "advise             90\t  54\t  51\t 144\t 141\t62.5%\t63.8%\t63.2%\n",
        "effect            160\t  98\t 152\t 258\t 312\t62.0%\t51.3%\t56.1%\n",
        "int                14\t   5\t  14\t  19\t  28\t73.7%\t50.0%\t59.6%\n",
        "mechanism          64\t  53\t 197\t 117\t 261\t54.7%\t24.5%\t33.9%\n",
        "------------------------------------------------------------------------------\n",
        "M.avg            -\t-\t-\t-\t-\t63.2%\t47.4%\t53.2%\n",
        "------------------------------------------------------------------------------\n",
        "m.avg             328\t 210\t 414\t 538\t 742\t61.0%\t44.2%\t51.2%\n",
        "m.avg(no class)   359\t 179\t 383\t 538\t 742\t66.7%\t48.4%\t56.1%\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "v9IJjNoBI--j"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "MUDLab6-DDI_lower_POS tagging.ipynb",
      "provenance": [],
      "mount_file_id": "1h5U2v8X1ObVeIljJMEpfPVkUVi3L7h8G",
      "authorship_tag": "ABX9TyO5poZmMIpFvqnPBlnoIpd8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}